{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d83409",
   "metadata": {},
   "source": [
    "# ðŸ¤– Multi-Agent RAG System\n",
    "\n",
    "\"A truly multi-agentic AI Retrieval-Augmented Generation system where each agent is an independent entity with its own LLM instance, specialized configuration, and autonomous decision-making capabilities.\"\n",
    "\n",
    "## ðŸŽ¯ Multi-Agent Architecture\n",
    "\n",
    "**What Makes This Truly Multi-Agentic:**\n",
    "- **8 Independent Agents**: Each with its own dedicated LLM instance (not shared!)\n",
    "- **Specialized Configurations**: Different models (GPT-4o, GPT-3.5) and temperatures per agent\n",
    "- **Inter-Agent Communication**: Agents send messages and coordinate via message passing\n",
    "- **Autonomous Decision-Making**: Each agent independently evaluates and decides\n",
    "- **Performance Tracking**: Real-time metrics for each agent's success rate and decision history\n",
    "\n",
    "## ðŸ¤– The Agent Team\n",
    "\n",
    "1. **SecurityGuard** (GPT-4o, T=0.1) â†’ Adversarial threat detection\n",
    "2. **QueryOptimizer** (GPT-4o, T=0.4) â†’ Creative NLP optimization\n",
    "3. **DocumentRetriever** (GPT-3.5, T=0.0) â†’ Vector search specialist  \n",
    "4. **AnswerGenerator** (GPT-4o, T=0.3) â†’ Deep reasoning synthesis\n",
    "5. **GroundingValidator** (GPT-4o, T=0.1) â†’ Fact-checking validator\n",
    "6. **QualityEvaluator** (GPT-4o, T=0.2) â†’ Metacognitive assessment\n",
    "7. **OutputGuard** (GPT-4o, T=0.1) â†’ Final safety validation\n",
    "8. **MemoryManager** (GPT-3.5, T=0.0) â†’ Context management\n",
    "\n",
    "## ðŸš€ Setup Instructions\n",
    "1. **Install Dependencies**: `pip install -r requirements.txt`\n",
    "2. **Set API Key**: Create `.env` file with `OPENAI_API_KEY=your-key-here`\n",
    "3. **Run App**: `streamlit run app.py`\n",
    "\n",
    "**Important**: Never commit API keys to version control!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396de3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\\n\n",
    "from dotenv import load_dotenv\\n\n",
    "\\n\n",
    "# Load environment variables from .env file\\n\n",
    "load_dotenv()\\n\n",
    "\\n\n",
    "# Get API key from environment - NEVER hardcode keys!\\n\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\\n\n",
    "\\n\n",
    "if not OPENAI_API_KEY:\\n\n",
    "    print('âš ï¸ Please set OPENAI_API_KEY in your .env file')\\n\n",
    "else:\\n\n",
    "    print('âœ… API key loaded from environment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e327c",
   "metadata": {},
   "source": [
    "## ðŸ“„ Complete Multi-Agent Application\n",
    "\n",
    "This notebook demonstrates the multi-agent architecture. For the full application:\n",
    "\n",
    "- **Run**: `streamlit run app.py` to start the web interface\n",
    "- **Upload**: Add your documents through the sidebar\n",
    "- **Chat**: Ask questions and watch the agents collaborate!\n",
    "- **Monitor**: View agent performance metrics in real-time\n",
    "\n",
    "## ðŸ”„ True Multi-Agent System Features\n",
    "\n",
    "**Each Agent is Independent:**\n",
    "- Has its own LLM instance (ChatOpenAI object)\n",
    "- Uses specialized temperature and model configuration\n",
    "- Maintains its own decision history and performance metrics\n",
    "- Makes autonomous decisions based on its expertise\n",
    "\n",
    "**Agent Coordination:**\n",
    "- Agents send messages to each other via `send_message()`\n",
    "- Agents receive and process messages from other agents\n",
    "- Confidence scores tracked per agent\n",
    "- Performance monitoring (calls, successes, failures)\n",
    "\n",
    "**LangGraph Orchestration:**\n",
    "- Each workflow node wraps an independent agent\n",
    "- State flows through agents with inter-agent communication\n",
    "- QualityEvaluator autonomously decides on refinement\n",
    "- Conditional edges based on agent decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97582f9",
   "metadata": {},
   "source": [
    "## ðŸ§ª Demonstrating Multi-Agent Independence\n",
    "\n",
    "Let's verify that each agent has its own LLM instance with specialized configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b2f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the multi-agent system\n",
    "from agents import MultiAgentSystem, BaseAgent\n",
    "\n",
    "# Create the multi-agent system\n",
    "agent_system = MultiAgentSystem(retriever=None)\n",
    "\n",
    "# Demonstrate that each agent has its own LLM instance\n",
    "print(\"ðŸ¤– MULTI-AGENT SYSTEM ARCHITECTURE\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for agent_name, agent in agent_system.agents.items():\n",
    "    print(f\"\\n{agent_name.upper()}\")\n",
    "    print(f\"  â”œâ”€ Agent Name: {agent.name}\")\n",
    "    print(f\"  â”œâ”€ Model: {agent.model}\")\n",
    "    print(f\"  â”œâ”€ Temperature: {agent.temperature}\")\n",
    "    print(f\"  â”œâ”€ LLM Instance ID: {id(agent.llm)}\")\n",
    "    print(f\"  â””â”€ System Prompt: {agent.system_prompt[:70]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nâœ… Each agent has a UNIQUE LLM instance (different IDs)\")\n",
    "print(\"âœ… Each agent has SPECIALIZED configuration (model/temperature)\")\n",
    "print(\"âœ… Each agent has its own DECISION HISTORY and METRICS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c7e18",
   "metadata": {},
   "source": [
    "# ðŸ¤– Agentic RAG System\n",
    "\n",
    "A sophisticated multi-agent Retrieval-Augmented Generation system that demonstrates true agentic behavior through autonomous decision-making, multi-step reasoning, and quality validation.\n",
    "\n",
    "## ðŸŽ¯ Key Features\n",
    "- **Multi-Agent Architecture**: Coordinated agents for input validation, query rewriting, retrieval, generation, and quality control\n",
    "- **Document Support**: PDF, DOCX, PPT, Excel, TXT files \n",
    "- **Advanced Retrieval**: MMR (Maximal Marginal Relevance) search with FAISS vector store\n",
    "- **Safety & Quality**: Input guards, grounding validation, output filtering\n",
    "- **Memory Management**: Conversation history and context preservation\n",
    "\n",
    "## ðŸš€ Workflow Steps\n",
    "1. **Input Guard** â†’ Security validation\n",
    "2. **Query Rewriting** â†’ LLM-powered optimization  \n",
    "3. **Document Retrieval** â†’ Vector similarity search\n",
    "4. **Answer Generation** â†’ Context-based responses\n",
    "5. **Grounding Validation** â†’ Accuracy verification\n",
    "6. **Output Guard** â†’ Final safety checks\n",
    "7. **Memory Update** â†’ History management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d82d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredPowerPointLoader\n",
    ")\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    docs = []\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        path = os.path.join(folder_path, file)\n",
    "\n",
    "        try:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                loader = PyPDFLoader(path)\n",
    "                docs.extend(loader.load())\n",
    "\n",
    "            elif file.endswith(\".docx\"):\n",
    "                loader = UnstructuredWordDocumentLoader(path)\n",
    "                docs.extend(loader.load())\n",
    "\n",
    "            elif file.endswith(\".pptx\"):\n",
    "                loader = UnstructuredPowerPointLoader(path)\n",
    "                docs.extend(loader.load())\n",
    "\n",
    "            elif file.endswith(\".txt\"):\n",
    "                loader = TextLoader(path)\n",
    "                docs.extend(loader.load())\n",
    "\n",
    "            elif file.endswith(\".xlsx\"):\n",
    "                df = pd.read_excel(path)\n",
    "                docs.append(Document(\n",
    "                    page_content=df.to_string(),\n",
    "                    metadata={\"source\": file, \"type\": \"excel\"}\n",
    "                ))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c011984",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation & Setup\n",
    "\n",
    "Install required dependencies and import necessary libraries for the agentic RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "735de1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_documents(docs):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    return splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ce678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load API key from environment variables - NEVER hardcode API keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set. Please configure your API key.\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize chat model  \n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a7e33",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration\n",
    "\n",
    "Configure LLM and embedding models with your API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73e44046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def build_faiss_store(chunks):\n",
    "    return FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21795427",
   "metadata": {},
   "source": [
    "## ðŸ—‚ï¸ Vector Store & Retrieval\n",
    "\n",
    "Setup FAISS vector database and document retrieval components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c34c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(vectorstore):\n",
    "    return vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 5}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee66587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    chat_history: List[str]\n",
    "    rewritten_question: str\n",
    "    context: List[str]\n",
    "    answer: str\n",
    "    iteration_count: int\n",
    "    evaluation_feedback: str\n",
    "    needs_refinement: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933226e",
   "metadata": {},
   "source": [
    "## ðŸ¤– Multi-Agent Architecture\n",
    "\n",
    "Define the agentic workflow components and state management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6328484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_guard(state: AgentState):\n",
    "    blocked_patterns = [\"ignore previous instructions\", \"jailbreak\"]\n",
    "\n",
    "    if any(p in state[\"question\"].lower() for p in blocked_patterns):\n",
    "        return {\"answer\": \"Query blocked due to unsafe instruction.\"}\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6242abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(state: AgentState):\n",
    "    # Check if this is a refinement iteration\n",
    "    is_refinement = state.get('iteration_count', 0) > 0\n",
    "    feedback = state.get('evaluation_feedback', '')\n",
    "    \n",
    "    if is_refinement and feedback:\n",
    "        prompt = f\"\"\"\n",
    "        The previous answer was evaluated and needs improvement. Rewrite the query to address the identified issues:\n",
    "\n",
    "        Original Question: {state['question']}\n",
    "        Previous Rewritten Query: {state.get('rewritten_question', state['question'])}\n",
    "        Evaluation Feedback: {feedback}\n",
    "        \n",
    "        Create a more targeted and specific query that addresses the feedback and should retrieve better context for answering the original question.\n",
    "        Focus on being more specific, using different keywords, or approaching the topic from a different angle.\n",
    "        \n",
    "        Improved Query:\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "        Rewrite the following question for optimal semantic retrieval:\n",
    "\n",
    "        {state['question']}\n",
    "        \n",
    "        Make it more specific and focused to retrieve the most relevant information.\n",
    "        \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\"rewritten_question\": response.content.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f7621ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(state: AgentState):\n",
    "    docs = retriever.invoke(state[\"rewritten_question\"])\n",
    "    return {\"context\": [doc.page_content for doc in docs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "164081fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state: AgentState):\n",
    "    history = \"\\n\".join(state[\"chat_history\"])\n",
    "    context = \"\\n\\n\".join(state[\"context\"])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Answer the question using the provided context. Use your knowledge to provide helpful responses based on the context.\n",
    "    If context contains relevant information, provide a comprehensive answer.\n",
    "    Only say \"Not found in documents\" if the context is completely unrelated to the question.\n",
    "\n",
    "    Conversation History:\n",
    "    {history}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {state['question']}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d85218e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grounding_check(state: AgentState):\n",
    "    # Practical grounding validation - only blocks clearly hallucinated content\n",
    "    answer = state['answer']\n",
    "    \n",
    "    # If answer says \"Not found in documents\", it's already safe\n",
    "    if \"not found in documents\" in answer.lower():\n",
    "        return state\n",
    "    \n",
    "    # Simple check: if answer is very short, it's likely factual\n",
    "    if len(answer) < 200:\n",
    "        return state\n",
    "        \n",
    "    # For longer answers, do a basic validation\n",
    "    validation_prompt = f\"\"\"\n",
    "    Does the answer contain any information that directly contradicts or is completely unrelated to the provided context?\n",
    "    Only respond \"CONTRADICTS\" if there are clear factual errors or completely made-up information.\n",
    "    Otherwise respond \"ACCEPTABLE\".\n",
    "\n",
    "    Context: {str(state['context'])[:600]}...\n",
    "    Answer: {answer}\n",
    "    \"\"\"\n",
    "\n",
    "    result = llm.invoke(validation_prompt)\n",
    "\n",
    "    if \"CONTRADICTS\" in result.content:\n",
    "        return {\"answer\": \"Answer contains information that may not be accurate.\"}\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77349cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_guard(state: AgentState):\n",
    "    banned_words = [\"confidential\", \"private data\"]\n",
    "\n",
    "    if any(word in state[\"answer\"].lower() for word in banned_words):\n",
    "        return {\"answer\": \"Response blocked due to policy violation.\"}\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "25738281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory(state: AgentState):\n",
    "    updated_history = state[\"chat_history\"] + [\n",
    "        f\"User: {state['question']}\",\n",
    "        f\"Assistant: {state['answer']}\"\n",
    "    ]\n",
    "    return {\"chat_history\": updated_history}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "36cdaede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(state: AgentState):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of the generated answer and determine if refinement is needed.\n",
    "    Uses criteria like relevance, completeness, accuracy, and clarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"\n",
    "    You are an expert evaluator. Assess the quality of the following answer to determine if it needs refinement.\n",
    "\n",
    "    Original Question: {state['question']}\n",
    "    Rewritten Query: {state['rewritten_question']}\n",
    "    Generated Answer: {state['answer']}\n",
    "    Current Iteration: {state.get('iteration_count', 0)}\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    1. Relevance: Does the answer directly address the question?\n",
    "    2. Completeness: Are all aspects of the question covered?\n",
    "    3. Accuracy: Is the information factually correct based on the context?\n",
    "    4. Clarity: Is the answer clear and well-structured?\n",
    "    5. Specificity: Does it provide concrete details rather than vague responses?\n",
    "\n",
    "    Based on these criteria, determine:\n",
    "    - If the answer is satisfactory (ACCEPT)\n",
    "    - If the answer needs improvement and can benefit from query refinement (REFINE)\n",
    "\n",
    "    Respond with:\n",
    "    DECISION: [ACCEPT/REFINE]\n",
    "    FEEDBACK: [Brief explanation of what needs improvement if REFINE, or confirmation if ACCEPT]\n",
    "\n",
    "    Note: Only suggest REFINE if there are clear improvement opportunities and we haven't reached max iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(evaluation_prompt)\n",
    "    evaluation_result = response.content\n",
    "\n",
    "    # Parse the evaluation result\n",
    "    lines = evaluation_result.strip().split('\\n')\n",
    "    decision = \"ACCEPT\"  # Default to accept\n",
    "    feedback = \"Answer meets quality standards.\"\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.startswith(\"DECISION:\"):\n",
    "            decision = line.split(\":\", 1)[1].strip()\n",
    "        elif line.startswith(\"FEEDBACK:\"):\n",
    "            feedback = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "    # Determine if refinement is needed\n",
    "    current_iteration = state.get('iteration_count', 0)\n",
    "    max_iterations = 2\n",
    "    \n",
    "    needs_refinement = (\n",
    "        decision == \"REFINE\" and \n",
    "        current_iteration < max_iterations and\n",
    "        len(state.get('answer', '')) > 0  # Only refine if we have an answer\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"evaluation_feedback\": feedback,\n",
    "        \"needs_refinement\": needs_refinement,\n",
    "        \"iteration_count\": current_iteration + (1 if needs_refinement else 0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a317c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Enhanced workflow with evaluation loop (max 2 iterations)\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add all agents including the new evaluator\n",
    "workflow.add_node(\"input_guard\", input_guard)\n",
    "workflow.add_node(\"rewrite\", rewrite_query)\n",
    "workflow.add_node(\"retrieve\", retrieve_docs)\n",
    "workflow.add_node(\"generate\", generate_answer)\n",
    "workflow.add_node(\"grounding\", grounding_check)\n",
    "workflow.add_node(\"evaluate\", evaluate_answer)  # New evaluation agent\n",
    "workflow.add_node(\"output_guard\", output_guard)\n",
    "workflow.add_node(\"memory\", update_memory)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"input_guard\")\n",
    "\n",
    "# Define workflow edges\n",
    "workflow.add_edge(\"input_guard\", \"rewrite\")\n",
    "workflow.add_edge(\"rewrite\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"grounding\")\n",
    "workflow.add_edge(\"grounding\", \"evaluate\")  # Add evaluation step\n",
    "\n",
    "# Conditional edges for iterative refinement\n",
    "def should_refine(state):\n",
    "    \"\"\"Determine if we should loop back for refinement or proceed to output\"\"\"\n",
    "    if state.get(\"needs_refinement\", False):\n",
    "        return \"rewrite\"  # Loop back to query rewriting\n",
    "    else:\n",
    "        return \"output_guard\"  # Proceed to final output\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"evaluate\",\n",
    "    should_refine,\n",
    "    {\n",
    "        \"rewrite\": \"rewrite\",      # Loop back for refinement\n",
    "        \"output_guard\": \"output_guard\"  # Proceed to end\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"output_guard\", \"memory\")\n",
    "workflow.add_edge(\"memory\", END)\n",
    "\n",
    "# Compile the enhanced workflow\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1a0eac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_state(question: str) -> AgentState:\n",
    "    \"\"\"Create an initial state for the enhanced workflow with iteration tracking\"\"\"\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"chat_history\": [],\n",
    "        \"rewritten_question\": \"\",\n",
    "        \"context\": [],\n",
    "        \"answer\": \"\",\n",
    "        \"iteration_count\": 0,\n",
    "        \"evaluation_feedback\": \"\",\n",
    "        \"needs_refinement\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaff1ad",
   "metadata": {},
   "source": [
    "## ðŸ”— Workflow Assembly\n",
    "\n",
    "Compile the multi-agent workflow into an executable graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95e388c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF TO USE FOR THIS RAG APP FOR NOW IS\n",
    "\n",
    "pdf_path = r\"C:\\Users\\gulsh\\Downloads\\Agentic-RAG\\jkssb-constable-advt-12of2025-24122025_6953b95e7e9af executive.pdf\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa363a",
   "metadata": {},
   "source": [
    "## ðŸ“„ Document Processing\n",
    "\n",
    "Initialize the knowledge base with your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7e61833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Loaded 15 pages from PDF\n",
      "Chunking documents...\n",
      "Created 47 chunks\n",
      "Building FAISS vector store...\n",
      "Vector store created successfully\n",
      "Creating retriever...\n",
      "Retriever ready!\n"
     ]
    }
   ],
   "source": [
    "# Load and process the PDF document\n",
    "print(\"Loading PDF...\")\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} pages from PDF\")\n",
    "\n",
    "# Chunk the documents\n",
    "print(\"Chunking documents...\")\n",
    "chunks = chunk_documents(docs)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "# Build vector store\n",
    "print(\"Building FAISS vector store...\")\n",
    "vectorstore = build_faiss_store(chunks)\n",
    "print(\"Vector store created successfully\")\n",
    "\n",
    "# Create retriever\n",
    "print(\"Creating retriever...\")\n",
    "retriever = get_retriever(vectorstore)\n",
    "print(\"Retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b826c",
   "metadata": {},
   "source": [
    "## ðŸš€ Demo & Testing\n",
    "\n",
    "Test the agentic RAG system with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "829682e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Testing Enhanced Agentic RAG System with Iterative Refinement\n",
      "======================================================================\n",
      "\n",
      "ðŸ” Question 1: What is JKSSB?\n",
      "--------------------------------------------------\n",
      "ðŸ¤– Final Answer: The Jammu & Kashmir Services Selection Board (JKSSB) is an organization responsible for conducting recruitment processes for various posts within the government departments of Jammu and Kashmir. It invites applications from eligible candidates and oversees the selection process to ensure fairness and transparency.\n",
      "ðŸ”„ Iterations Performed: 0\n",
      "ðŸ“Š Final Evaluation: No feedback available\n",
      "âœ… Answer accepted on first attempt\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ðŸ” Question 2: How do I apply?\n",
      "--------------------------------------------------\n",
      "ðŸ¤– Final Answer: To apply, you need to fill out the application form online as no other means or mode of application will be accepted. Here are the steps you should follow:\n",
      "\n",
      "1. Visit the official website of the Board where the necessary instructions and details of the Online Portal will be made available.\n",
      "2. Carefully fill out the application form online. Make sure to review the form diligently before submitting it, as once submitted, changes cannot be made.\n",
      "3. Pay the application fee online using Net Banking, Credit, or Debit cards. The fee is Rs. 700 for general candidates and Rs. 600 for candidates belonging to SC, ST-1, ST-2, and EWS categories.\n",
      "4. In-service candidates need to submit a printout of the filled online application form along with a duly filled, signed, and stamped certificate through the proper channel via the concerned Head of Department/Designated Authority.\n",
      "5. Keep checking the official website for updates and information about the examination, including details about downloading Admit Cards, which will be available about one week before the date of the examination.\n",
      "ðŸ”„ Iterations Performed: 0\n",
      "ðŸ“Š Final Evaluation: No feedback available\n",
      "âœ… Answer accepted on first attempt\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ðŸ” Question 3: What are the qualifications needed?\n",
      "--------------------------------------------------\n",
      "ðŸ¤– Final Answer: The qualifications needed for the advertised posts are as follows:\n",
      "\n",
      "1. Academic Qualification: Matriculation.\n",
      "2. Physical Standard Test & Physical Endurance Test: Details are mentioned in Annexure â€œBâ€ of the notification.\n",
      "3. For candidates claiming possession of equivalent educational qualifications, they must provide relevant certificates as proof.\n",
      "\n",
      "Additionally, candidates must meet specific physical standards:\n",
      "- For Males: Minimum height of 5â€™-6â€, chest girth of 32â€ (unexpanded) and 33 Â½â€ (expanded).\n",
      "- For Females: Minimum height of 5â€™-2â€.\n",
      "- Height relaxations are provided for candidates belonging to the Gorkha Community and the Bot Tribe by 2 inches.\n",
      "\n",
      "Candidates must produce relevant certificates in original as proof of having acquired the prescribed educational qualification on or before the cut-off date for filling online application forms.\n",
      "ðŸ”„ Iterations Performed: 0\n",
      "ðŸ“Š Final Evaluation: No feedback available\n",
      "âœ… Answer accepted on first attempt\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ðŸ” Question 4: Tell me about the selection process\n",
      "--------------------------------------------------\n",
      "ðŸ¤– Final Answer: The selection process comprises several stages:\n",
      "\n",
      "1. **Written Test**: Candidates will undergo a Written Examination to determine merit. Those with NCC Certificates will receive Bonus Marks as per the provisions in S.O. 290 of 2021.\n",
      "\n",
      "2. **Physical Standard Test (PST)**: Candidates, based on their merit, will take the PST, which is qualifying in nature. The number of candidates selected for the PST will be six times the total number of vacancies in each category.\n",
      "\n",
      "3. **Physical Endurance Test (PET)**: Candidates who qualify in the PST will then undergo the PET, which is also qualifying in nature.\n",
      "\n",
      "Additionally, candidates must participate in Document Verification, bringing original certificates and valid ID proof. No TA/DA will be provided for participation in the written test or document verification. The selection process also includes considerations for Horizontal Reservation, but Persons with Benchmark Disabilities (PwBD) are not eligible for these posts.\n",
      "ðŸ”„ Iterations Performed: 0\n",
      "ðŸ“Š Final Evaluation: No feedback available\n",
      "âœ… Answer accepted on first attempt\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "ðŸŽ‰ Enhanced Agentic RAG System Demo Complete!\n",
      "\n",
      "ðŸ“ˆ System Features Demonstrated:\n",
      "   â€¢ âœ… Autonomous decision making\n",
      "   â€¢ ðŸ”„ Iterative query refinement (max 2 iterations)\n",
      "   â€¢ ðŸŽ¯ Answer quality evaluation\n",
      "   â€¢ ðŸ” Multi-step reasoning\n",
      "   â€¢ ðŸ›¡ï¸ Safety controls\n",
      "   â€¢ ðŸ’­ Self-reflection and improvement\n",
      "\n",
      "ðŸš€ Ready for advanced agentic applications!\n"
     ]
    }
   ],
   "source": [
    "# Test the Enhanced Agentic RAG System with Evaluation Loop\n",
    "print(\"ðŸŽ¯ Testing Enhanced Agentic RAG System with Iterative Refinement\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test questions designed to potentially trigger refinement\n",
    "test_questions = [\n",
    "    \"What is JKSSB?\",  # Simple question - likely to be accepted\n",
    "    \"How do I apply?\",  # Potentially ambiguous - might need refinement\n",
    "    \"What are the qualifications needed?\",  # Complex question - might benefit from refinement\n",
    "    \"Tell me about the selection process\"  # Broad question - likely to need refinement\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nðŸ” Question {i}: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create enhanced state with iteration tracking\n",
    "    initial_state = create_enhanced_state(question)\n",
    "    \n",
    "    # Run through the enhanced agentic workflow with evaluation loop\n",
    "    result = app.invoke(initial_state)\n",
    "    \n",
    "    # Display results with iteration information\n",
    "    final_answer = result[\"answer\"]\n",
    "    iterations = result.get(\"iteration_count\", 0)\n",
    "    evaluation_feedback = result.get(\"evaluation_feedback\", \"No feedback available\")\n",
    "    \n",
    "    print(f\"ðŸ¤– Final Answer: {final_answer}\")\n",
    "    print(f\"ðŸ”„ Iterations Performed: {iterations}\")\n",
    "    print(f\"ðŸ“Š Final Evaluation: {evaluation_feedback}\")\n",
    "    \n",
    "    if iterations > 0:\n",
    "        print(f\"âœ¨ Query was refined {iterations} time(s) for better results!\")\n",
    "    else:\n",
    "        print(\"âœ… Answer accepted on first attempt\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nðŸŽ‰ Enhanced Agentic RAG System Demo Complete!\")\n",
    "print(\"\\nðŸ“ˆ System Features Demonstrated:\")  \n",
    "print(\"   â€¢ âœ… Autonomous decision making\")  \n",
    "print(\"   â€¢ ðŸ”„ Iterative query refinement (max 2 iterations)\")\n",
    "print(\"   â€¢ ðŸŽ¯ Answer quality evaluation\") \n",
    "print(\"   â€¢ ðŸ” Multi-step reasoning\")\n",
    "print(\"   â€¢ ðŸ›¡ï¸ Safety controls\")\n",
    "print(\"   â€¢ ðŸ’­ Self-reflection and improvement\")\n",
    "print(\"\\nðŸš€ Ready for advanced agentic applications!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dbb9f5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Single Question Test - Iterative Refinement Demo\n",
      "============================================================\n",
      "ðŸ” Test Question: How do I apply?\n",
      "----------------------------------------\n",
      "ðŸ¤– Final Answer: To apply, you need to fill out the application form online. The necessary instructions for filling out the online application, including details about the online portal, will be available on the Board's website. You must submit the application form in online mode only, as no other means or modes of application will be accepted. Additionally, in-service candidates must submit a printout of the filled online application form along with a duly filled, signed, and stamped certificate through the proper channel, which is the concerned Head of Department or Designated Authority.\n",
      "ðŸ”„ Total Iterations: 0\n",
      "ðŸ“Š Evaluation Feedback: No feedback\n",
      "ðŸŽ¯ Rewritten Query: What are the steps and requirements for submitting an application?\n",
      "\n",
      "âœ… Answer was satisfactory on the first attempt!\n",
      "\n",
      "ðŸŽ‰ Iterative refinement demonstration complete!\n"
     ]
    }
   ],
   "source": [
    "# Test single question to demonstrate iterative refinement\n",
    "print(\"ðŸ§ª Single Question Test - Iterative Refinement Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_question = \"How do I apply?\"  # Ambiguous question likely to need refinement\n",
    "\n",
    "print(f\"ðŸ” Test Question: {test_question}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create enhanced state\n",
    "initial_state = create_enhanced_state(test_question)\n",
    "\n",
    "# Run through enhanced workflow \n",
    "result = app.invoke(initial_state)\n",
    "\n",
    "# Display detailed results\n",
    "print(f\"ðŸ¤– Final Answer: {result['answer']}\")\n",
    "print(f\"ðŸ”„ Total Iterations: {result.get('iteration_count', 0)}\")\n",
    "print(f\"ðŸ“Š Evaluation Feedback: {result.get('evaluation_feedback', 'No feedback')}\")\n",
    "print(f\"ðŸŽ¯ Rewritten Query: {result.get('rewritten_question', 'N/A')}\")\n",
    "\n",
    "if result.get('iteration_count', 0) > 0:\n",
    "    print(f\"\\nâœ¨ System performed {result['iteration_count']} refinement(s) to improve the answer!\")\n",
    "    print(\"ðŸ”„ This demonstrates autonomous self-improvement capabilities!\")\n",
    "else:\n",
    "    print(\"\\nâœ… Answer was satisfactory on the first attempt!\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Iterative refinement demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e88d83",
   "metadata": {},
   "source": [
    "## ðŸŽŠ Enhanced System Summary\n",
    "\n",
    "### Architecture Highlights\n",
    "- **8-Agent Pipeline**: Input validation â†’ Query rewriting â†’ Document retrieval â†’ Answer generation â†’ Grounding validation â†’ **Answer evaluation** â†’ Output filtering â†’ Memory update\n",
    "- **Iterative Refinement**: Self-evaluating system with up to 2 refinement iterations for optimal answers\n",
    "- **True Agentic Behavior**: Autonomous decision-making, multi-step reasoning, self-reflection, and iterative improvement\n",
    "- **Production Ready**: Comprehensive safety controls, quality validation, and intelligent loop-back mechanism\n",
    "- **Scalable Design**: Supports multiple document formats and can be extended with additional agents\n",
    "\n",
    "### Key Technologies\n",
    "- **LangGraph**: Multi-agent orchestration with conditional workflows\n",
    "- **FAISS**: High-performance vector search\n",
    "- **OpenAI GPT-4o**: Advanced language understanding and evaluation\n",
    "- **LangChain**: Document processing and workflow management\n",
    "\n",
    "### Enhanced Workflow Features\n",
    "ðŸ”„ **Iterative Refinement Loop**\n",
    "- Automatic answer quality evaluation\n",
    "- Intelligent query rewriting based on feedback\n",
    "- Maximum 2 iterations to prevent infinite loops\n",
    "- Self-improving system that gets better answers through iteration\n",
    "\n",
    "ðŸŽ¯ **Smart Evaluation Criteria**\n",
    "- Relevance to original question\n",
    "- Completeness of information\n",
    "- Accuracy and factual correctness\n",
    "- Clarity and structure\n",
    "- Specificity vs vague responses\n",
    "\n",
    "### Usage\n",
    "```python\n",
    "# Enhanced usage with iteration tracking\n",
    "initial_state = create_enhanced_state(\"Your question here\")\n",
    "result = app.invoke(initial_state)\n",
    "\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Iterations: {result['iteration_count']}\")\n",
    "print(f\"Evaluation: {result['evaluation_feedback']}\")\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "1. **Scale Up**: Add more documents to the knowledge base\n",
    "2. **Extend**: Add specialized agents for different domains  \n",
    "3. **Optimize**: Fine-tune evaluation criteria and iteration limits\n",
    "4. **Advanced Features**: Add user feedback loops and learning mechanisms\n",
    "5. **Deploy**: Integrate with web applications or APIs\n",
    "\n",
    "**ðŸš€ Your Enhanced Agentic RAG System with Iterative Refinement is ready for advanced applications!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
